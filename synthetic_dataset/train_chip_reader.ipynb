{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 150, 4)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Import all the Keras machinery we need\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "from gen_data import generate_chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789/'\n",
    "label_d = {}\n",
    "for index, k in enumerate(labels):\n",
    "    label_d[k] = index\n",
    "#print(label_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input():\n",
    "\n",
    "    chip, coords, letters, letter_size = generate_chip()\n",
    "    \n",
    "    return chip, coords, letters, letter_size\n",
    "\n",
    "def get_output(letter, label_file=None):\n",
    "\n",
    "    # img_id = path.split('/')[-1].split('.')[0]\n",
    "    # img_id = np.int64(img_id)\n",
    "    # labels = label_file.loc[img_id].values\n",
    "\n",
    "    return(label_d[letter])\n",
    "\n",
    "def preprocess_input(chip, coord, letter_size):\n",
    "\n",
    "    image = chip.crop((coord[0], coord[1], coord[0]+letter_size[0], coord[1]+letter_size[1]))\n",
    "    im_resized = image.resize((224,224))\n",
    "    im_final  = im_resized.convert('RGB')\n",
    "    im_final = np.array(im_final)\n",
    "    #print(im_final.shape)\n",
    "    #print(im_resized.mode)\n",
    "\n",
    "    return(im_final)\n",
    "\n",
    "def image_generator(batch_size):\n",
    "\n",
    "    while True:\n",
    "        # Select files (paths/indices) for the batch\n",
    "        # batch_paths = np.random.choice(a = files,\n",
    "        #                                  size = batch_size)\n",
    "        batch_input = []\n",
    "        batch_output = []\n",
    "\n",
    "        # Read in each input, perform preprocessing and get labels\n",
    "        for i in range(batch_size):\n",
    "            chip, coords, letters, letter_size = get_input()\n",
    "            #print(letters)\n",
    "            for i in range(4):#len(letters)):\n",
    "                output = get_output(letters[i])\n",
    "                input = preprocess_input(chip, coords[i], letter_size)\n",
    "                batch_input += [ input ]\n",
    "                batch_output += [ output ]\n",
    "\n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "        batch_x = np.array( batch_input )\n",
    "        batch_y = np.array( batch_output )\n",
    "    \n",
    "        print('BATCH GENERATED')\n",
    "        \n",
    "        \n",
    "        yield( batch_x, to_categorical(batch_y) )\n",
    "        \n",
    "def image_generator2(batch_size):\n",
    "\n",
    "    while True:\n",
    "        # Select files (paths/indices) for the batch\n",
    "        # batch_paths = np.random.choice(a = files,\n",
    "        #                                  size = batch_size)\n",
    "\n",
    "    \n",
    "              \n",
    "        \n",
    "        chip, coords, letters, letter_size = get_input()\n",
    "        step = len(letters) // batch_size\n",
    "        print(step)\n",
    "\n",
    "        for i in range(step):\n",
    "            batch_input = []\n",
    "            batch_output = []\n",
    "\n",
    "            # Return a tuple of (input,output) to feed the network\n",
    "\n",
    "\n",
    "            for i in range(len(letters)):\n",
    "                output = get_output(letters[i])\n",
    "                input = preprocess_input(chip, coords[i], letter_size)\n",
    "                batch_input += [ input ]\n",
    "                batch_output += [ output ]\n",
    "            batch_x = np.array( batch_input )\n",
    "            batch_y = np.array( batch_output )   \n",
    "            print('BATCH GENERATED') \n",
    "            #print('batch_x',batch_x)\n",
    "            yield( batch_x, to_categorical(batch_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = 200\n",
    "n_val_samples =  1\n",
    "batch_size = 1\n",
    "num_classes = 37\n",
    "\n",
    "\n",
    "train_generator = image_generator2(batch_size)\n",
    "\n",
    "validation_generator = image_generator2(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Build a simple CONVNET\\n# Like in https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\\nfrom keras.models import Sequential\\nfrom keras.layers import Conv2D, MaxPooling2D\\nfrom keras.layers import Activation, Dropout, Flatten, Dense\\n\\nif K.image_data_format() == 'channels_first':\\n    input_shape = (3, 224, 224)\\nelse:\\n    input_shape = (224, 224, 3)\\n\\ncnnmodel = Sequential()\\ncnnmodel.add(Conv2D(32, (3, 3), input_shape=input_shape))\\ncnnmodel.add(Activation('relu'))\\ncnnmodel.add(MaxPooling2D(pool_size=(2, 2)))\\n\\n\\ncnnmodel.add(Conv2D(64, (3, 3)))\\ncnnmodel.add(Activation('relu'))\\ncnnmodel.add(MaxPooling2D(pool_size=(2, 2)))\\n\\n\\n\\n\\ncnnmodel.add(Flatten())\\ncnnmodel.add(Dense(64))\\ncnnmodel.add(Activation('relu'))\\n\\n\\ncnnmodel.add(Dropout(0.5))\\ncnnmodel.add(Dense(num_classes))\\ncnnmodel.add(Activation('softmax'))\\n\\n\\n\\ncnnmodel.compile(loss='categorical_crossentropy',\\n                optimizer=optimizers.Adadelta(),\\n                metrics=['accuracy'])\\n\\n#K.clear_session()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Build a simple CONVNET\n",
    "# Like in https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, 224, 224)\n",
    "else:\n",
    "    input_shape = (224, 224, 3)\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "cnnmodel.add(Activation('relu'))\n",
    "cnnmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "cnnmodel.add(Conv2D(64, (3, 3)))\n",
    "cnnmodel.add(Activation('relu'))\n",
    "cnnmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cnnmodel.add(Flatten())\n",
    "cnnmodel.add(Dense(64))\n",
    "cnnmodel.add(Activation('relu'))\n",
    "\n",
    "\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "cnnmodel.add(Dense(num_classes))\n",
    "cnnmodel.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizers.Adadelta(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "#K.clear_session()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abert/.virtualenvs/test/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abert/.virtualenvs/test/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 56, 56, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 56, 56, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 28, 28, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100352)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fc360 (Dense)                   (None, 37)           3713061     flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,300,773\n",
      "Trainable params: 27,247,653\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "model_name = 'gros_resnet50'\n",
    "\n",
    "# number of classes\n",
    "\n",
    "# input image shape\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# load base model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False,\n",
    "                  input_shape=input_shape)\n",
    "\n",
    "# append classification layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "final_output = Dense(num_classes, activation='softmax', name='fc360')(x)\n",
    "\n",
    "# create the new model\n",
    "model = Model(inputs=base_model.input, outputs=final_output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizers.Adam(lr = 1e-4),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# set the first n layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "#for layer in model.layers[:10]:\n",
    "#    layer.trainable = False\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abert/.virtualenvs/test/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "18\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "BATCH GENERATED\n",
      "  1/200 [..............................] - ETA: 30:26 - loss: 4.8525 - acc: 0.0556BATCH GENERATED\n",
      "  2/200 [..............................] - ETA: 16:40 - loss: 2.4263 - acc: 0.5278BATCH GENERATED\n",
      "  3/200 [..............................] - ETA: 12:04 - loss: 1.6175 - acc: 0.6852BATCH GENERATED\n",
      "  4/200 [..............................] - ETA: 9:46 - loss: 1.2131 - acc: 0.7639 BATCH GENERATED\n",
      "  5/200 [..............................] - ETA: 8:22 - loss: 0.9705 - acc: 0.8111BATCH GENERATED\n",
      "  6/200 [..............................] - ETA: 7:26 - loss: 0.8088 - acc: 0.8426BATCH GENERATED\n",
      "  7/200 [>.............................] - ETA: 6:46 - loss: 0.6932 - acc: 0.865118\n",
      "BATCH GENERATED\n",
      "  8/200 [>.............................] - ETA: 6:16 - loss: 0.6066 - acc: 0.8819BATCH GENERATED\n",
      "  9/200 [>.............................] - ETA: 5:52 - loss: 0.5392 - acc: 0.8951BATCH GENERATED\n",
      " 10/200 [>.............................] - ETA: 5:32 - loss: 0.4853 - acc: 0.9056BATCH GENERATED\n",
      " 11/200 [>.............................] - ETA: 5:16 - loss: 0.4411 - acc: 0.9141BATCH GENERATED\n",
      " 12/200 [>.............................] - ETA: 5:03 - loss: 0.4044 - acc: 0.9213BATCH GENERATED\n",
      " 13/200 [>.............................] - ETA: 4:51 - loss: 0.3733 - acc: 0.9274BATCH GENERATED\n",
      " 14/200 [=>............................] - ETA: 4:41 - loss: 0.3466 - acc: 0.9325BATCH GENERATED\n",
      " 15/200 [=>............................] - ETA: 4:33 - loss: 0.3235 - acc: 0.9370BATCH GENERATED\n",
      " 16/200 [=>............................] - ETA: 4:25 - loss: 0.3033 - acc: 0.9410BATCH GENERATED\n",
      " 17/200 [=>............................] - ETA: 4:18 - loss: 0.2854 - acc: 0.9444BATCH GENERATED\n",
      " 18/200 [=>............................] - ETA: 4:11 - loss: 0.2696 - acc: 0.9475BATCH GENERATED\n",
      " 19/200 [=>............................] - ETA: 4:06 - loss: 0.5881 - acc: 0.9240BATCH GENERATED\n",
      " 20/200 [==>...........................] - ETA: 4:00 - loss: 0.8274 - acc: 0.9111BATCH GENERATED\n",
      " 21/200 [==>...........................] - ETA: 3:56 - loss: 1.0438 - acc: 0.8995BATCH GENERATED\n",
      " 22/200 [==>...........................] - ETA: 3:51 - loss: 1.2406 - acc: 0.8889BATCH GENERATED\n",
      " 23/200 [==>...........................] - ETA: 3:47 - loss: 1.4202 - acc: 0.8792BATCH GENERATED\n",
      " 24/200 [==>...........................] - ETA: 3:43 - loss: 1.5849 - acc: 0.8704BATCH GENERATED\n",
      " 25/200 [==>...........................] - ETA: 3:39 - loss: 1.7364 - acc: 0.862218\n",
      "BATCH GENERATED\n",
      " 26/200 [==>...........................] - ETA: 3:36 - loss: 1.8763 - acc: 0.8547BATCH GENERATED\n",
      " 27/200 [===>..........................] - ETA: 3:33 - loss: 2.0058 - acc: 0.8477BATCH GENERATED\n",
      " 28/200 [===>..........................] - ETA: 3:29 - loss: 2.1260 - acc: 0.8413BATCH GENERATED\n",
      " 29/200 [===>..........................] - ETA: 3:27 - loss: 2.2380 - acc: 0.8352BATCH GENERATED\n",
      " 30/200 [===>..........................] - ETA: 3:24 - loss: 2.3425 - acc: 0.8296BATCH GENERATED\n",
      " 31/200 [===>..........................] - ETA: 3:21 - loss: 2.4402 - acc: 0.8244BATCH GENERATED\n",
      " 32/200 [===>..........................] - ETA: 3:18 - loss: 2.5319 - acc: 0.8194BATCH GENERATED\n",
      " 33/200 [===>..........................] - ETA: 3:16 - loss: 2.6179 - acc: 0.8148BATCH GENERATED\n",
      " 34/200 [====>.........................] - ETA: 3:14 - loss: 2.6990 - acc: 0.8105BATCH GENERATED\n",
      " 35/200 [====>.........................] - ETA: 3:11 - loss: 2.7754 - acc: 0.8063BATCH GENERATED\n",
      " 36/200 [====>.........................] - ETA: 3:09 - loss: 2.8475 - acc: 0.8025BATCH GENERATED\n",
      " 37/200 [====>.........................] - ETA: 3:07 - loss: 2.9400 - acc: 0.7973BATCH GENERATED\n",
      " 38/200 [====>.........................] - ETA: 3:05 - loss: 3.0276 - acc: 0.7924BATCH GENERATED\n",
      " 39/200 [====>.........................] - ETA: 3:03 - loss: 3.1106 - acc: 0.7877BATCH GENERATED\n",
      " 40/200 [=====>........................] - ETA: 3:01 - loss: 3.1896 - acc: 0.7833BATCH GENERATED\n",
      " 41/200 [=====>........................] - ETA: 2:59 - loss: 3.2647 - acc: 0.7791BATCH GENERATED\n",
      " 42/200 [=====>........................] - ETA: 2:57 - loss: 3.3362 - acc: 0.7751BATCH GENERATED\n",
      " 43/200 [=====>........................] - ETA: 2:55 - loss: 3.4044 - acc: 0.771318\n",
      "BATCH GENERATED\n",
      " 44/200 [=====>........................] - ETA: 2:53 - loss: 3.4695 - acc: 0.7677BATCH GENERATED\n",
      " 45/200 [=====>........................] - ETA: 2:52 - loss: 3.5316 - acc: 0.7642BATCH GENERATED\n",
      " 46/200 [=====>........................] - ETA: 2:50 - loss: 3.5911 - acc: 0.7609BATCH GENERATED\n",
      " 47/200 [======>.......................] - ETA: 2:48 - loss: 3.6481 - acc: 0.7577BATCH GENERATED\n",
      " 48/200 [======>.......................] - ETA: 2:46 - loss: 3.7027 - acc: 0.7546BATCH GENERATED\n",
      " 49/200 [======>.......................] - ETA: 2:45 - loss: 3.7550 - acc: 0.7517BATCH GENERATED\n",
      " 50/200 [======>.......................] - ETA: 2:43 - loss: 3.8053 - acc: 0.7489BATCH GENERATED\n",
      " 51/200 [======>.......................] - ETA: 2:42 - loss: 3.8536 - acc: 0.7462BATCH GENERATED\n",
      " 52/200 [======>.......................] - ETA: 2:40 - loss: 3.9000 - acc: 0.7436BATCH GENERATED\n",
      " 53/200 [======>.......................] - ETA: 2:39 - loss: 3.9447 - acc: 0.7411BATCH GENERATED\n",
      " 54/200 [=======>......................] - ETA: 2:37 - loss: 3.9877 - acc: 0.7387BATCH GENERATED\n",
      " 55/200 [=======>......................] - ETA: 2:36 - loss: 4.0481 - acc: 0.7343BATCH GENERATED\n",
      " 56/200 [=======>......................] - ETA: 2:34 - loss: 4.0885 - acc: 0.7321BATCH GENERATED\n",
      " 57/200 [=======>......................] - ETA: 2:33 - loss: 4.1267 - acc: 0.7300BATCH GENERATED\n",
      " 58/200 [=======>......................] - ETA: 2:31 - loss: 4.1637 - acc: 0.7280BATCH GENERATED\n",
      " 59/200 [=======>......................] - ETA: 2:30 - loss: 4.1993 - acc: 0.7260BATCH GENERATED\n",
      " 60/200 [========>.....................] - ETA: 2:28 - loss: 4.2338 - acc: 0.7241BATCH GENERATED\n",
      " 61/200 [========>.....................] - ETA: 2:27 - loss: 4.2672 - acc: 0.722218\n",
      "BATCH GENERATED\n",
      " 62/200 [========>.....................] - ETA: 2:26 - loss: 4.2994 - acc: 0.7204BATCH GENERATED\n",
      " 63/200 [========>.....................] - ETA: 2:24 - loss: 4.3307 - acc: 0.7187BATCH GENERATED\n",
      " 64/200 [========>.....................] - ETA: 2:23 - loss: 4.3610 - acc: 0.7170BATCH GENERATED\n",
      " 65/200 [========>.....................] - ETA: 2:22 - loss: 4.3903 - acc: 0.7154BATCH GENERATED\n",
      " 66/200 [========>.....................] - ETA: 2:20 - loss: 4.4187 - acc: 0.7138BATCH GENERATED\n",
      " 67/200 [=========>....................] - ETA: 2:19 - loss: 4.4463 - acc: 0.7123BATCH GENERATED\n",
      " 68/200 [=========>....................] - ETA: 2:18 - loss: 4.4729 - acc: 0.7108BATCH GENERATED\n",
      " 69/200 [=========>....................] - ETA: 2:17 - loss: 4.4861 - acc: 0.7101BATCH GENERATED\n",
      " 70/200 [=========>....................] - ETA: 2:15 - loss: 4.4988 - acc: 0.7095BATCH GENERATED\n",
      " 71/200 [=========>....................] - ETA: 2:14 - loss: 4.5111 - acc: 0.7089BATCH GENERATED\n",
      " 72/200 [=========>....................] - ETA: 2:13 - loss: 4.5230 - acc: 0.7083BATCH GENERATED\n",
      " 73/200 [=========>....................] - ETA: 2:12 - loss: 4.5230 - acc: 0.7070BATCH GENERATED\n",
      " 74/200 [==========>...................] - ETA: 2:10 - loss: 4.5103 - acc: 0.7080BATCH GENERATED\n",
      " 75/200 [==========>...................] - ETA: 2:09 - loss: 4.4979 - acc: 0.7089BATCH GENERATED\n",
      " 76/200 [==========>...................] - ETA: 2:08 - loss: 4.4858 - acc: 0.7098BATCH GENERATED\n",
      " 77/200 [==========>...................] - ETA: 2:07 - loss: 4.4741 - acc: 0.7107BATCH GENERATED\n",
      " 78/200 [==========>...................] - ETA: 2:05 - loss: 4.4627 - acc: 0.7115BATCH GENERATED\n",
      " 79/200 [==========>...................] - ETA: 2:04 - loss: 4.4515 - acc: 0.712418\n",
      "BATCH GENERATED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/200 [===========>..................] - ETA: 2:03 - loss: 4.4406 - acc: 0.7132BATCH GENERATED\n",
      " 81/200 [===========>..................] - ETA: 2:02 - loss: 4.4300 - acc: 0.7140BATCH GENERATED\n",
      " 82/200 [===========>..................] - ETA: 2:01 - loss: 4.4197 - acc: 0.7148BATCH GENERATED\n",
      " 83/200 [===========>..................] - ETA: 2:00 - loss: 4.4096 - acc: 0.7155BATCH GENERATED\n",
      " 84/200 [===========>..................] - ETA: 1:58 - loss: 4.3998 - acc: 0.7163BATCH GENERATED\n",
      " 85/200 [===========>..................] - ETA: 1:57 - loss: 4.3901 - acc: 0.7170BATCH GENERATED\n",
      " 86/200 [===========>..................] - ETA: 1:56 - loss: 4.3807 - acc: 0.7177BATCH GENERATED\n",
      " 87/200 [============>.................] - ETA: 1:55 - loss: 4.3715 - acc: 0.7184BATCH GENERATED\n",
      " 88/200 [============>.................] - ETA: 1:54 - loss: 4.3626 - acc: 0.7191BATCH GENERATED\n",
      " 89/200 [============>.................] - ETA: 1:53 - loss: 4.3538 - acc: 0.7197BATCH GENERATED\n",
      " 90/200 [============>.................] - ETA: 1:51 - loss: 4.3452 - acc: 0.7204BATCH GENERATED\n",
      " 91/200 [============>.................] - ETA: 1:50 - loss: 4.3430 - acc: 0.7204BATCH GENERATED\n",
      " 92/200 [============>.................] - ETA: 1:49 - loss: 4.3348 - acc: 0.7210BATCH GENERATED\n",
      " 93/200 [============>.................] - ETA: 1:48 - loss: 4.3267 - acc: 0.7216BATCH GENERATED\n",
      " 94/200 [=============>................] - ETA: 1:47 - loss: 4.3188 - acc: 0.7222BATCH GENERATED\n",
      " 95/200 [=============>................] - ETA: 1:46 - loss: 4.3110 - acc: 0.7228BATCH GENERATED\n",
      " 96/200 [=============>................] - ETA: 1:45 - loss: 4.3034 - acc: 0.7234BATCH GENERATED\n",
      " 97/200 [=============>................] - ETA: 1:44 - loss: 4.2960 - acc: 0.723918\n",
      "BATCH GENERATED\n",
      " 98/200 [=============>................] - ETA: 1:43 - loss: 4.2887 - acc: 0.7245BATCH GENERATED\n",
      " 99/200 [=============>................] - ETA: 1:41 - loss: 4.2816 - acc: 0.7250BATCH GENERATED\n",
      "100/200 [==============>...............] - ETA: 1:40 - loss: 4.2746 - acc: 0.7256BATCH GENERATED\n",
      "101/200 [==============>...............] - ETA: 1:39 - loss: 4.2677 - acc: 0.7261BATCH GENERATED\n",
      "102/200 [==============>...............] - ETA: 1:38 - loss: 4.2610 - acc: 0.7266BATCH GENERATED\n",
      "103/200 [==============>...............] - ETA: 1:37 - loss: 4.2544 - acc: 0.7271BATCH GENERATED\n",
      "104/200 [==============>...............] - ETA: 1:36 - loss: 4.2479 - acc: 0.7276BATCH GENERATED\n",
      "105/200 [==============>...............] - ETA: 1:35 - loss: 4.2416 - acc: 0.7280BATCH GENERATED\n",
      "106/200 [==============>...............] - ETA: 1:34 - loss: 4.2354 - acc: 0.7285BATCH GENERATED\n",
      "107/200 [===============>..............] - ETA: 1:33 - loss: 4.2293 - acc: 0.7290BATCH GENERATED\n",
      "108/200 [===============>..............] - ETA: 1:32 - loss: 4.2233 - acc: 0.7294BATCH GENERATED\n",
      "109/200 [===============>..............] - ETA: 1:31 - loss: 4.2081 - acc: 0.7299BATCH GENERATED\n",
      "110/200 [===============>..............] - ETA: 1:30 - loss: 4.1789 - acc: 0.7313BATCH GENERATED\n",
      "111/200 [===============>..............] - ETA: 1:28 - loss: 4.1493 - acc: 0.7332BATCH GENERATED\n",
      "112/200 [===============>..............] - ETA: 1:27 - loss: 4.1203 - acc: 0.7351BATCH GENERATED\n",
      "113/200 [===============>..............] - ETA: 1:26 - loss: 4.0917 - acc: 0.7370BATCH GENERATED\n",
      "114/200 [================>.............] - ETA: 1:25 - loss: 4.0637 - acc: 0.7388BATCH GENERATED\n",
      "115/200 [================>.............] - ETA: 1:24 - loss: 4.0362 - acc: 0.740618\n",
      "BATCH GENERATED\n",
      "116/200 [================>.............] - ETA: 1:23 - loss: 4.0091 - acc: 0.7423BATCH GENERATED\n",
      "117/200 [================>.............] - ETA: 1:22 - loss: 3.9825 - acc: 0.7441BATCH GENERATED\n",
      "118/200 [================>.............] - ETA: 1:21 - loss: 3.9563 - acc: 0.7458BATCH GENERATED\n",
      "119/200 [================>.............] - ETA: 1:20 - loss: 3.9306 - acc: 0.7474BATCH GENERATED\n",
      "120/200 [=================>............] - ETA: 1:19 - loss: 3.9053 - acc: 0.7491BATCH GENERATED\n",
      "121/200 [=================>............] - ETA: 1:18 - loss: 3.8804 - acc: 0.7507BATCH GENERATED\n",
      "122/200 [=================>............] - ETA: 1:17 - loss: 3.8560 - acc: 0.7523BATCH GENERATED\n",
      "123/200 [=================>............] - ETA: 1:16 - loss: 3.8319 - acc: 0.7538BATCH GENERATED\n",
      "124/200 [=================>............] - ETA: 1:15 - loss: 3.8082 - acc: 0.7554BATCH GENERATED\n",
      "125/200 [=================>............] - ETA: 1:14 - loss: 3.7849 - acc: 0.7569BATCH GENERATED\n",
      "126/200 [=================>............] - ETA: 1:13 - loss: 3.7620 - acc: 0.7584BATCH GENERATED\n",
      "127/200 [==================>...........] - ETA: 1:12 - loss: 3.7815 - acc: 0.7568BATCH GENERATED\n",
      "128/200 [==================>...........] - ETA: 1:11 - loss: 3.7852 - acc: 0.7565BATCH GENERATED\n",
      "129/200 [==================>...........] - ETA: 1:10 - loss: 3.7837 - acc: 0.7567BATCH GENERATED\n",
      "130/200 [==================>...........] - ETA: 1:09 - loss: 3.7821 - acc: 0.7568BATCH GENERATED\n",
      "131/200 [==================>...........] - ETA: 1:08 - loss: 3.7806 - acc: 0.7570BATCH GENERATED\n",
      "132/200 [==================>...........] - ETA: 1:07 - loss: 3.7791 - acc: 0.7572BATCH GENERATED\n",
      "133/200 [==================>...........] - ETA: 1:06 - loss: 3.7777 - acc: 0.757318\n",
      "BATCH GENERATED\n",
      "134/200 [===================>..........] - ETA: 1:05 - loss: 3.7762 - acc: 0.7575BATCH GENERATED\n",
      "135/200 [===================>..........] - ETA: 1:04 - loss: 3.7748 - acc: 0.7576BATCH GENERATED\n",
      "136/200 [===================>..........] - ETA: 1:03 - loss: 3.7640 - acc: 0.7582BATCH GENERATED\n",
      "137/200 [===================>..........] - ETA: 1:02 - loss: 3.7430 - acc: 0.7595BATCH GENERATED\n",
      "138/200 [===================>..........] - ETA: 1:01 - loss: 3.7224 - acc: 0.7609BATCH GENERATED\n",
      "139/200 [===================>..........] - ETA: 1:00 - loss: 3.7021 - acc: 0.7622BATCH GENERATED\n",
      "140/200 [====================>.........] - ETA: 59s - loss: 3.6820 - acc: 0.7635 BATCH GENERATED\n",
      "141/200 [====================>.........] - ETA: 58s - loss: 3.6623 - acc: 0.7648BATCH GENERATED\n",
      "142/200 [====================>.........] - ETA: 57s - loss: 3.6428 - acc: 0.7660BATCH GENERATED\n",
      "143/200 [====================>.........] - ETA: 55s - loss: 3.6236 - acc: 0.7673BATCH GENERATED\n",
      "144/200 [====================>.........] - ETA: 54s - loss: 3.6046 - acc: 0.7685BATCH GENERATED\n",
      "145/200 [====================>.........] - ETA: 53s - loss: 3.6141 - acc: 0.7670BATCH GENERATED\n",
      "146/200 [====================>.........] - ETA: 52s - loss: 3.6058 - acc: 0.7675BATCH GENERATED\n",
      "147/200 [=====================>........] - ETA: 51s - loss: 3.5908 - acc: 0.7683BATCH GENERATED\n",
      "148/200 [=====================>........] - ETA: 50s - loss: 3.5726 - acc: 0.7695BATCH GENERATED\n",
      "149/200 [=====================>........] - ETA: 49s - loss: 3.5546 - acc: 0.7707BATCH GENERATED\n",
      "150/200 [=====================>........] - ETA: 48s - loss: 3.5369 - acc: 0.7719BATCH GENERATED\n",
      "151/200 [=====================>........] - ETA: 47s - loss: 3.5194 - acc: 0.773018\n",
      "BATCH GENERATED\n",
      "152/200 [=====================>........] - ETA: 46s - loss: 3.5021 - acc: 0.7741BATCH GENERATED\n",
      "153/200 [=====================>........] - ETA: 45s - loss: 3.4851 - acc: 0.7752BATCH GENERATED\n",
      "154/200 [======================>.......] - ETA: 44s - loss: 3.4683 - acc: 0.7763BATCH GENERATED\n",
      "155/200 [======================>.......] - ETA: 43s - loss: 3.4517 - acc: 0.7774BATCH GENERATED\n",
      "156/200 [======================>.......] - ETA: 43s - loss: 3.4353 - acc: 0.7785BATCH GENERATED\n",
      "157/200 [======================>.......] - ETA: 42s - loss: 3.4191 - acc: 0.7795BATCH GENERATED\n",
      "158/200 [======================>.......] - ETA: 41s - loss: 3.4031 - acc: 0.7806BATCH GENERATED\n",
      "159/200 [======================>.......] - ETA: 40s - loss: 3.3874 - acc: 0.7816BATCH GENERATED\n",
      "160/200 [=======================>......] - ETA: 39s - loss: 3.3718 - acc: 0.7826BATCH GENERATED\n",
      "161/200 [=======================>......] - ETA: 38s - loss: 3.3564 - acc: 0.7836BATCH GENERATED\n",
      "162/200 [=======================>......] - ETA: 37s - loss: 3.3412 - acc: 0.7846BATCH GENERATED\n",
      "163/200 [=======================>......] - ETA: 36s - loss: 3.3341 - acc: 0.7843BATCH GENERATED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/200 [=======================>......] - ETA: 35s - loss: 3.3196 - acc: 0.7849BATCH GENERATED\n",
      "165/200 [=======================>......] - ETA: 34s - loss: 3.3049 - acc: 0.7859BATCH GENERATED\n",
      "166/200 [=======================>......] - ETA: 33s - loss: 3.2904 - acc: 0.7868BATCH GENERATED\n",
      "167/200 [========================>.....] - ETA: 32s - loss: 3.2761 - acc: 0.7878BATCH GENERATED\n",
      "168/200 [========================>.....] - ETA: 31s - loss: 3.2619 - acc: 0.7887BATCH GENERATED\n",
      "169/200 [========================>.....] - ETA: 30s - loss: 3.2479 - acc: 0.789618\n",
      "BATCH GENERATED\n",
      "170/200 [========================>.....] - ETA: 29s - loss: 3.2341 - acc: 0.7905BATCH GENERATED\n",
      "171/200 [========================>.....] - ETA: 28s - loss: 3.2204 - acc: 0.7914BATCH GENERATED\n",
      "172/200 [========================>.....] - ETA: 27s - loss: 3.2069 - acc: 0.7923BATCH GENERATED\n",
      "173/200 [========================>.....] - ETA: 26s - loss: 3.1935 - acc: 0.7932BATCH GENERATED\n",
      "174/200 [=========================>....] - ETA: 25s - loss: 3.1803 - acc: 0.7941BATCH GENERATED\n",
      "175/200 [=========================>....] - ETA: 24s - loss: 3.1672 - acc: 0.7949BATCH GENERATED\n",
      "176/200 [=========================>....] - ETA: 23s - loss: 3.1543 - acc: 0.7958BATCH GENERATED\n",
      "177/200 [=========================>....] - ETA: 22s - loss: 3.1416 - acc: 0.7966BATCH GENERATED\n",
      "178/200 [=========================>....] - ETA: 21s - loss: 3.1290 - acc: 0.7974BATCH GENERATED\n",
      "179/200 [=========================>....] - ETA: 20s - loss: 3.1165 - acc: 0.7983BATCH GENERATED\n",
      "180/200 [==========================>...] - ETA: 19s - loss: 3.1041 - acc: 0.7991BATCH GENERATED\n",
      "181/200 [==========================>...] - ETA: 18s - loss: 3.1086 - acc: 0.7986BATCH GENERATED\n",
      "182/200 [==========================>...] - ETA: 17s - loss: 3.1017 - acc: 0.7988BATCH GENERATED\n",
      "183/200 [==========================>...] - ETA: 16s - loss: 3.0896 - acc: 0.7996BATCH GENERATED\n",
      "184/200 [==========================>...] - ETA: 15s - loss: 3.0777 - acc: 0.8004BATCH GENERATED\n",
      "185/200 [==========================>...] - ETA: 14s - loss: 3.0659 - acc: 0.8012BATCH GENERATED\n",
      "186/200 [==========================>...] - ETA: 13s - loss: 3.0543 - acc: 0.8020BATCH GENERATED\n",
      "187/200 [===========================>..] - ETA: 12s - loss: 3.0427 - acc: 0.802718\n",
      "BATCH GENERATED\n",
      "188/200 [===========================>..] - ETA: 11s - loss: 3.0313 - acc: 0.8035BATCH GENERATED\n",
      "189/200 [===========================>..] - ETA: 10s - loss: 3.0200 - acc: 0.8042BATCH GENERATED\n",
      "190/200 [===========================>..] - ETA: 9s - loss: 3.0088 - acc: 0.8050 BATCH GENERATED\n",
      "191/200 [===========================>..] - ETA: 8s - loss: 2.9977 - acc: 0.8057BATCH GENERATED\n",
      "192/200 [===========================>..] - ETA: 7s - loss: 2.9868 - acc: 0.8064BATCH GENERATED\n",
      "193/200 [===========================>..] - ETA: 6s - loss: 2.9760 - acc: 0.8071BATCH GENERATED\n",
      "194/200 [============================>.] - ETA: 5s - loss: 2.9652 - acc: 0.8078BATCH GENERATED\n",
      "195/200 [============================>.] - ETA: 4s - loss: 2.9546 - acc: 0.8085BATCH GENERATED\n",
      "196/200 [============================>.] - ETA: 3s - loss: 2.9441 - acc: 0.8092BATCH GENERATED\n",
      "197/200 [============================>.] - ETA: 2s - loss: 2.9337 - acc: 0.8099BATCH GENERATED\n",
      "198/200 [============================>.] - ETA: 1s - loss: 2.9234 - acc: 0.8106BATCH GENERATED\n",
      "199/200 [============================>.] - ETA: 0s - loss: 2.9218 - acc: 0.8102BATCH GENERATED\n",
      "200/200 [==============================] - 194s 971ms/step - loss: 2.9166 - acc: 0.8106\n",
      "Epoch 2/2\n",
      "BATCH GENERATED\n",
      "  1/200 [..............................] - ETA: 3:16 - loss: 1.8404 - acc: 0.8889BATCH GENERATED\n",
      "  2/200 [..............................] - ETA: 3:12 - loss: 1.8192 - acc: 0.8889BATCH GENERATED\n",
      "  3/200 [..............................] - ETA: 3:12 - loss: 1.8099 - acc: 0.8889BATCH GENERATED\n",
      "  4/200 [..............................] - ETA: 3:11 - loss: 1.8052 - acc: 0.8889BATCH GENERATED\n",
      "  5/200 [..............................] - ETA: 3:09 - loss: 1.8025 - acc: 0.888918\n",
      "BATCH GENERATED\n",
      "  6/200 [..............................] - ETA: 3:09 - loss: 1.8007 - acc: 0.8889BATCH GENERATED\n",
      "  7/200 [>.............................] - ETA: 3:08 - loss: 1.7993 - acc: 0.8889BATCH GENERATED\n",
      "  8/200 [>.............................] - ETA: 3:07 - loss: 1.7983 - acc: 0.8889BATCH GENERATED\n",
      "  9/200 [>.............................] - ETA: 3:06 - loss: 1.7975 - acc: 0.8889BATCH GENERATED\n",
      " 10/200 [>.............................] - ETA: 3:05 - loss: 1.7968 - acc: 0.8889BATCH GENERATED\n",
      " 11/200 [>.............................] - ETA: 3:04 - loss: 1.7963 - acc: 0.8889BATCH GENERATED\n",
      " 12/200 [>.............................] - ETA: 3:03 - loss: 1.7959 - acc: 0.8889BATCH GENERATED\n",
      " 13/200 [>.............................] - ETA: 3:02 - loss: 1.7955 - acc: 0.8889BATCH GENERATED\n",
      " 14/200 [=>............................] - ETA: 3:01 - loss: 1.7952 - acc: 0.8889BATCH GENERATED\n",
      " 15/200 [=>............................] - ETA: 2:59 - loss: 1.7949 - acc: 0.8889BATCH GENERATED\n",
      " 16/200 [=>............................] - ETA: 2:58 - loss: 1.7946 - acc: 0.8889BATCH GENERATED\n",
      " 17/200 [=>............................] - ETA: 2:57 - loss: 1.9959 - acc: 0.8725BATCH GENERATED\n",
      " 18/200 [=>............................] - ETA: 2:55 - loss: 2.1391 - acc: 0.8642BATCH GENERATED\n",
      " 19/200 [=>............................] - ETA: 2:54 - loss: 2.2542 - acc: 0.8567BATCH GENERATED\n",
      " 20/200 [==>...........................] - ETA: 2:53 - loss: 2.3260 - acc: 0.8500BATCH GENERATED\n",
      " 21/200 [==>...........................] - ETA: 2:51 - loss: 2.3858 - acc: 0.8466BATCH GENERATED\n",
      " 22/200 [==>...........................] - ETA: 2:50 - loss: 2.4402 - acc: 0.8434BATCH GENERATED\n",
      " 23/200 [==>...........................] - ETA: 2:49 - loss: 2.4898 - acc: 0.840618\n",
      "BATCH GENERATED\n",
      " 24/200 [==>...........................] - ETA: 2:48 - loss: 2.5353 - acc: 0.8380BATCH GENERATED\n",
      " 25/200 [==>...........................] - ETA: 2:47 - loss: 2.5772 - acc: 0.8356BATCH GENERATED\n",
      " 26/200 [==>...........................] - ETA: 2:46 - loss: 2.6158 - acc: 0.8333BATCH GENERATED\n",
      " 27/200 [===>..........................] - ETA: 2:45 - loss: 2.6516 - acc: 0.8313BATCH GENERATED\n",
      " 28/200 [===>..........................] - ETA: 2:43 - loss: 2.6848 - acc: 0.8294BATCH GENERATED\n",
      " 29/200 [===>..........................] - ETA: 2:42 - loss: 2.7158 - acc: 0.8276BATCH GENERATED\n",
      " 30/200 [===>..........................] - ETA: 2:41 - loss: 2.7446 - acc: 0.8259BATCH GENERATED\n",
      " 31/200 [===>..........................] - ETA: 2:40 - loss: 2.7716 - acc: 0.8244BATCH GENERATED\n",
      " 32/200 [===>..........................] - ETA: 2:39 - loss: 2.7970 - acc: 0.8229BATCH GENERATED\n",
      " 33/200 [===>..........................] - ETA: 2:38 - loss: 2.8207 - acc: 0.8215BATCH GENERATED\n",
      " 34/200 [====>.........................] - ETA: 2:37 - loss: 2.8431 - acc: 0.8203BATCH GENERATED\n",
      " 35/200 [====>.........................] - ETA: 2:36 - loss: 2.8663 - acc: 0.8190BATCH GENERATED\n",
      " 36/200 [====>.........................] - ETA: 2:35 - loss: 2.8862 - acc: 0.8179BATCH GENERATED\n",
      " 37/200 [====>.........................] - ETA: 2:34 - loss: 2.9050 - acc: 0.8168BATCH GENERATED\n",
      " 38/200 [====>.........................] - ETA: 2:33 - loss: 2.9228 - acc: 0.8158BATCH GENERATED\n",
      " 39/200 [====>.........................] - ETA: 2:32 - loss: 2.9397 - acc: 0.8148BATCH GENERATED\n",
      " 40/200 [=====>........................] - ETA: 2:31 - loss: 2.9557 - acc: 0.8139BATCH GENERATED\n",
      " 41/200 [=====>........................] - ETA: 2:30 - loss: 2.9710 - acc: 0.813018\n",
      "BATCH GENERATED\n",
      " 42/200 [=====>........................] - ETA: 2:29 - loss: 2.9855 - acc: 0.8122BATCH GENERATED\n",
      " 43/200 [=====>........................] - ETA: 2:28 - loss: 2.9994 - acc: 0.8114BATCH GENERATED\n",
      " 44/200 [=====>........................] - ETA: 2:27 - loss: 3.0126 - acc: 0.8106BATCH GENERATED\n",
      " 45/200 [=====>........................] - ETA: 2:26 - loss: 3.0253 - acc: 0.8099BATCH GENERATED\n",
      " 46/200 [=====>........................] - ETA: 2:25 - loss: 3.0374 - acc: 0.8092BATCH GENERATED\n",
      " 47/200 [======>.......................] - ETA: 2:24 - loss: 3.0490 - acc: 0.8085BATCH GENERATED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48/200 [======>.......................] - ETA: 2:23 - loss: 3.0601 - acc: 0.8079BATCH GENERATED\n",
      " 49/200 [======>.......................] - ETA: 2:22 - loss: 3.0707 - acc: 0.8073BATCH GENERATED\n",
      " 50/200 [======>.......................] - ETA: 2:21 - loss: 3.0809 - acc: 0.8067BATCH GENERATED\n",
      " 51/200 [======>.......................] - ETA: 2:20 - loss: 3.0908 - acc: 0.8061BATCH GENERATED\n",
      " 52/200 [======>.......................] - ETA: 2:19 - loss: 3.1002 - acc: 0.8056BATCH GENERATED\n",
      " 53/200 [======>.......................] - ETA: 2:18 - loss: 3.0784 - acc: 0.8061BATCH GENERATED\n",
      " 54/200 [=======>......................] - ETA: 2:17 - loss: 3.0545 - acc: 0.8076BATCH GENERATED\n",
      " 55/200 [=======>......................] - ETA: 2:16 - loss: 3.0316 - acc: 0.8091BATCH GENERATED\n",
      " 56/200 [=======>......................] - ETA: 2:15 - loss: 3.0094 - acc: 0.8105BATCH GENERATED\n",
      " 57/200 [=======>......................] - ETA: 2:14 - loss: 2.9880 - acc: 0.8119BATCH GENERATED\n",
      " 58/200 [=======>......................] - ETA: 2:13 - loss: 2.9674 - acc: 0.8132BATCH GENERATED\n",
      " 59/200 [=======>......................] - ETA: 2:12 - loss: 2.9474 - acc: 0.814518\n",
      "BATCH GENERATED\n",
      " 60/200 [========>.....................] - ETA: 2:11 - loss: 2.9282 - acc: 0.8157BATCH GENERATED\n",
      " 61/200 [========>.....................] - ETA: 2:10 - loss: 2.9095 - acc: 0.8169BATCH GENERATED\n",
      " 62/200 [========>.....................] - ETA: 2:09 - loss: 2.8915 - acc: 0.8181BATCH GENERATED\n",
      " 63/200 [========>.....................] - ETA: 2:08 - loss: 2.8740 - acc: 0.8192BATCH GENERATED\n",
      " 64/200 [========>.....................] - ETA: 2:07 - loss: 2.8571 - acc: 0.8203BATCH GENERATED\n",
      " 65/200 [========>.....................] - ETA: 2:06 - loss: 2.8407 - acc: 0.8214BATCH GENERATED\n",
      " 66/200 [========>.....................] - ETA: 2:05 - loss: 2.8248 - acc: 0.8224BATCH GENERATED\n",
      " 67/200 [=========>....................] - ETA: 2:04 - loss: 2.8094 - acc: 0.8234BATCH GENERATED\n",
      " 68/200 [=========>....................] - ETA: 2:03 - loss: 2.7944 - acc: 0.8243BATCH GENERATED\n",
      " 69/200 [=========>....................] - ETA: 2:02 - loss: 2.7798 - acc: 0.8253BATCH GENERATED\n",
      " 70/200 [=========>....................] - ETA: 2:01 - loss: 2.7657 - acc: 0.8262BATCH GENERATED\n",
      " 71/200 [=========>....................] - ETA: 2:00 - loss: 2.7541 - acc: 0.8263BATCH GENERATED\n",
      " 72/200 [=========>....................] - ETA: 1:59 - loss: 2.7408 - acc: 0.8272BATCH GENERATED\n",
      " 73/200 [=========>....................] - ETA: 1:58 - loss: 2.7278 - acc: 0.8280BATCH GENERATED\n",
      " 74/200 [==========>...................] - ETA: 1:57 - loss: 2.7049 - acc: 0.8288BATCH GENERATED\n",
      " 75/200 [==========>...................] - ETA: 1:56 - loss: 2.6808 - acc: 0.8304BATCH GENERATED\n",
      " 76/200 [==========>...................] - ETA: 1:55 - loss: 2.6573 - acc: 0.8319BATCH GENERATED\n",
      " 77/200 [==========>...................] - ETA: 1:55 - loss: 2.6344 - acc: 0.833318\n",
      "BATCH GENERATED\n",
      " 78/200 [==========>...................] - ETA: 1:54 - loss: 2.6121 - acc: 0.8348BATCH GENERATED\n",
      " 79/200 [==========>...................] - ETA: 1:53 - loss: 2.5904 - acc: 0.8361BATCH GENERATED\n",
      " 80/200 [===========>..................] - ETA: 1:52 - loss: 2.5692 - acc: 0.8375BATCH GENERATED\n",
      " 81/200 [===========>..................] - ETA: 1:51 - loss: 2.5485 - acc: 0.8388BATCH GENERATED\n",
      " 82/200 [===========>..................] - ETA: 1:50 - loss: 2.5284 - acc: 0.8401BATCH GENERATED\n",
      " 83/200 [===========>..................] - ETA: 1:49 - loss: 2.5087 - acc: 0.8414BATCH GENERATED\n",
      " 84/200 [===========>..................] - ETA: 1:48 - loss: 2.4895 - acc: 0.8426BATCH GENERATED\n",
      " 85/200 [===========>..................] - ETA: 1:47 - loss: 2.4708 - acc: 0.8438BATCH GENERATED\n",
      " 86/200 [===========>..................] - ETA: 1:46 - loss: 2.4524 - acc: 0.8450BATCH GENERATED\n",
      " 87/200 [============>.................] - ETA: 1:45 - loss: 2.4345 - acc: 0.8461BATCH GENERATED\n",
      " 88/200 [============>.................] - ETA: 1:44 - loss: 2.4171 - acc: 0.8472BATCH GENERATED\n",
      " 89/200 [============>.................] - ETA: 1:43 - loss: 2.4274 - acc: 0.8464BATCH GENERATED\n",
      " 90/200 [============>.................] - ETA: 1:42 - loss: 2.4303 - acc: 0.8463BATCH GENERATED\n",
      " 91/200 [============>.................] - ETA: 1:41 - loss: 2.4331 - acc: 0.8462BATCH GENERATED\n",
      " 92/200 [============>.................] - ETA: 1:40 - loss: 2.4358 - acc: 0.8460BATCH GENERATED\n",
      " 93/200 [============>.................] - ETA: 1:39 - loss: 2.4385 - acc: 0.8459BATCH GENERATED\n",
      " 94/200 [=============>................] - ETA: 1:38 - loss: 2.4412 - acc: 0.8457BATCH GENERATED\n",
      " 95/200 [=============>................] - ETA: 1:37 - loss: 2.4437 - acc: 0.845618\n",
      "BATCH GENERATED\n",
      " 96/200 [=============>................] - ETA: 1:36 - loss: 2.4463 - acc: 0.8455BATCH GENERATED\n",
      " 97/200 [=============>................] - ETA: 1:36 - loss: 2.4487 - acc: 0.8454BATCH GENERATED\n",
      " 98/200 [=============>................] - ETA: 1:35 - loss: 2.4512 - acc: 0.8452BATCH GENERATED\n",
      " 99/200 [=============>................] - ETA: 1:34 - loss: 2.4535 - acc: 0.8451BATCH GENERATED\n",
      "100/200 [==============>...............] - ETA: 1:33 - loss: 2.4559 - acc: 0.8450BATCH GENERATED\n",
      "101/200 [==============>...............] - ETA: 1:32 - loss: 2.4582 - acc: 0.8449BATCH GENERATED\n",
      "102/200 [==============>...............] - ETA: 1:31 - loss: 2.4604 - acc: 0.8448BATCH GENERATED\n",
      "103/200 [==============>...............] - ETA: 1:30 - loss: 2.4626 - acc: 0.8447BATCH GENERATED\n",
      "104/200 [==============>...............] - ETA: 1:29 - loss: 2.4647 - acc: 0.8446BATCH GENERATED\n",
      "105/200 [==============>...............] - ETA: 1:28 - loss: 2.4669 - acc: 0.8444BATCH GENERATED\n",
      "106/200 [==============>...............] - ETA: 1:27 - loss: 2.4689 - acc: 0.8443BATCH GENERATED\n",
      "107/200 [===============>..............] - ETA: 1:26 - loss: 2.4654 - acc: 0.8437BATCH GENERATED\n",
      "108/200 [===============>..............] - ETA: 1:25 - loss: 2.4592 - acc: 0.8441BATCH GENERATED\n",
      "109/200 [===============>..............] - ETA: 1:24 - loss: 2.4531 - acc: 0.8445BATCH GENERATED\n",
      "110/200 [===============>..............] - ETA: 1:23 - loss: 2.4470 - acc: 0.8449BATCH GENERATED\n",
      "111/200 [===============>..............] - ETA: 1:22 - loss: 2.4411 - acc: 0.8453BATCH GENERATED\n",
      "112/200 [===============>..............] - ETA: 1:21 - loss: 2.4353 - acc: 0.8457BATCH GENERATED\n",
      "113/200 [===============>..............] - ETA: 1:20 - loss: 2.4296 - acc: 0.846118\n",
      "BATCH GENERATED\n",
      "114/200 [================>.............] - ETA: 1:20 - loss: 2.4240 - acc: 0.8465BATCH GENERATED\n",
      "115/200 [================>.............] - ETA: 1:19 - loss: 2.4185 - acc: 0.8469BATCH GENERATED\n",
      "116/200 [================>.............] - ETA: 1:18 - loss: 2.4131 - acc: 0.8472BATCH GENERATED\n",
      "117/200 [================>.............] - ETA: 1:17 - loss: 2.4078 - acc: 0.8476BATCH GENERATED\n",
      "118/200 [================>.............] - ETA: 1:16 - loss: 2.4026 - acc: 0.8479BATCH GENERATED\n",
      "119/200 [================>.............] - ETA: 1:15 - loss: 2.3974 - acc: 0.8483BATCH GENERATED\n",
      "120/200 [=================>............] - ETA: 1:14 - loss: 2.3924 - acc: 0.8486BATCH GENERATED\n",
      "121/200 [=================>............] - ETA: 1:13 - loss: 2.3874 - acc: 0.8489BATCH GENERATED\n",
      "122/200 [=================>............] - ETA: 1:12 - loss: 2.3825 - acc: 0.8493BATCH GENERATED\n",
      "123/200 [=================>............] - ETA: 1:11 - loss: 2.3777 - acc: 0.8496BATCH GENERATED\n",
      "124/200 [=================>............] - ETA: 1:10 - loss: 2.3730 - acc: 0.8499BATCH GENERATED\n",
      "125/200 [=================>............] - ETA: 1:09 - loss: 2.3794 - acc: 0.8484BATCH GENERATED\n",
      "126/200 [=================>............] - ETA: 1:08 - loss: 2.3818 - acc: 0.8483BATCH GENERATED\n",
      "127/200 [==================>...........] - ETA: 1:07 - loss: 2.3842 - acc: 0.8482BATCH GENERATED\n",
      "128/200 [==================>...........] - ETA: 1:06 - loss: 2.3866 - acc: 0.8481BATCH GENERATED\n",
      "129/200 [==================>...........] - ETA: 1:05 - loss: 2.3889 - acc: 0.8480BATCH GENERATED\n",
      "130/200 [==================>...........] - ETA: 1:05 - loss: 2.3912 - acc: 0.8479BATCH GENERATED\n",
      "131/200 [==================>...........] - ETA: 1:04 - loss: 2.3935 - acc: 0.847818\n",
      "BATCH GENERATED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/200 [==================>...........] - ETA: 1:03 - loss: 2.3957 - acc: 0.8476BATCH GENERATED\n",
      "133/200 [==================>...........] - ETA: 1:02 - loss: 2.3979 - acc: 0.8475BATCH GENERATED\n",
      "134/200 [===================>..........] - ETA: 1:01 - loss: 2.4001 - acc: 0.8474BATCH GENERATED\n",
      "135/200 [===================>..........] - ETA: 1:00 - loss: 2.4022 - acc: 0.8473BATCH GENERATED\n",
      "136/200 [===================>..........] - ETA: 59s - loss: 2.4043 - acc: 0.8472 BATCH GENERATED\n",
      "137/200 [===================>..........] - ETA: 58s - loss: 2.4064 - acc: 0.8471BATCH GENERATED\n",
      "138/200 [===================>..........] - ETA: 57s - loss: 2.4084 - acc: 0.8470BATCH GENERATED\n",
      "139/200 [===================>..........] - ETA: 56s - loss: 2.4104 - acc: 0.8469BATCH GENERATED\n",
      "140/200 [====================>.........] - ETA: 55s - loss: 2.4124 - acc: 0.8468BATCH GENERATED\n",
      "141/200 [====================>.........] - ETA: 54s - loss: 2.4143 - acc: 0.8467BATCH GENERATED\n",
      "142/200 [====================>.........] - ETA: 53s - loss: 2.4162 - acc: 0.8466BATCH GENERATED\n",
      "143/200 [====================>.........] - ETA: 52s - loss: 2.4122 - acc: 0.8469BATCH GENERATED\n",
      "144/200 [====================>.........] - ETA: 52s - loss: 2.4079 - acc: 0.8472BATCH GENERATED\n",
      "145/200 [====================>.........] - ETA: 51s - loss: 2.4036 - acc: 0.8475BATCH GENERATED\n",
      "146/200 [====================>.........] - ETA: 50s - loss: 2.3994 - acc: 0.8478BATCH GENERATED\n",
      "147/200 [=====================>........] - ETA: 49s - loss: 2.3953 - acc: 0.8481BATCH GENERATED\n",
      "148/200 [=====================>........] - ETA: 48s - loss: 2.3912 - acc: 0.8483BATCH GENERATED\n",
      "149/200 [=====================>........] - ETA: 47s - loss: 2.3872 - acc: 0.848618\n",
      "BATCH GENERATED\n",
      "150/200 [=====================>........] - ETA: 46s - loss: 2.3832 - acc: 0.8489BATCH GENERATED\n",
      "151/200 [=====================>........] - ETA: 45s - loss: 2.3793 - acc: 0.8492BATCH GENERATED\n",
      "152/200 [=====================>........] - ETA: 44s - loss: 2.3754 - acc: 0.8494BATCH GENERATED\n",
      "153/200 [=====================>........] - ETA: 43s - loss: 2.3716 - acc: 0.8497BATCH GENERATED\n",
      "154/200 [======================>.......] - ETA: 42s - loss: 2.3678 - acc: 0.8499BATCH GENERATED\n",
      "155/200 [======================>.......] - ETA: 41s - loss: 2.3641 - acc: 0.8502BATCH GENERATED\n",
      "156/200 [======================>.......] - ETA: 41s - loss: 2.3604 - acc: 0.8504BATCH GENERATED\n",
      "157/200 [======================>.......] - ETA: 40s - loss: 2.3568 - acc: 0.8507BATCH GENERATED\n",
      "158/200 [======================>.......] - ETA: 39s - loss: 2.3532 - acc: 0.8509BATCH GENERATED\n",
      "159/200 [======================>.......] - ETA: 38s - loss: 2.3497 - acc: 0.8512BATCH GENERATED\n",
      "160/200 [=======================>......] - ETA: 37s - loss: 2.3462 - acc: 0.8514BATCH GENERATED\n",
      "161/200 [=======================>......] - ETA: 36s - loss: 2.3547 - acc: 0.8506BATCH GENERATED\n",
      "162/200 [=======================>......] - ETA: 35s - loss: 2.3623 - acc: 0.8501BATCH GENERATED\n",
      "163/200 [=======================>......] - ETA: 34s - loss: 2.3698 - acc: 0.8497BATCH GENERATED\n",
      "164/200 [=======================>......] - ETA: 33s - loss: 2.3772 - acc: 0.8493BATCH GENERATED\n",
      "165/200 [=======================>......] - ETA: 32s - loss: 2.3845 - acc: 0.8488BATCH GENERATED\n",
      "166/200 [=======================>......] - ETA: 31s - loss: 2.3917 - acc: 0.8484BATCH GENERATED\n",
      "167/200 [========================>.....] - ETA: 30s - loss: 2.3988 - acc: 0.848018\n",
      "BATCH GENERATED\n",
      "168/200 [========================>.....] - ETA: 29s - loss: 2.4059 - acc: 0.8476BATCH GENERATED\n",
      "169/200 [========================>.....] - ETA: 28s - loss: 2.4128 - acc: 0.8471BATCH GENERATED\n",
      "170/200 [========================>.....] - ETA: 27s - loss: 2.4197 - acc: 0.8467BATCH GENERATED\n",
      "171/200 [========================>.....] - ETA: 27s - loss: 2.4265 - acc: 0.8463BATCH GENERATED\n",
      "172/200 [========================>.....] - ETA: 26s - loss: 2.4332 - acc: 0.8459BATCH GENERATED\n",
      "173/200 [========================>.....] - ETA: 25s - loss: 2.4399 - acc: 0.8455BATCH GENERATED\n",
      "174/200 [=========================>....] - ETA: 24s - loss: 2.4464 - acc: 0.8451BATCH GENERATED\n",
      "175/200 [=========================>....] - ETA: 23s - loss: 2.4529 - acc: 0.8448BATCH GENERATED\n",
      "176/200 [=========================>....] - ETA: 22s - loss: 2.4593 - acc: 0.8444BATCH GENERATED\n",
      "177/200 [=========================>....] - ETA: 21s - loss: 2.4657 - acc: 0.8440BATCH GENERATED\n",
      "178/200 [=========================>....] - ETA: 20s - loss: 2.4719 - acc: 0.8436BATCH GENERATED\n",
      "179/200 [=========================>....] - ETA: 19s - loss: 2.4601 - acc: 0.8442BATCH GENERATED\n",
      "180/200 [==========================>...] - ETA: 18s - loss: 2.4465 - acc: 0.8451BATCH GENERATED\n",
      "181/200 [==========================>...] - ETA: 17s - loss: 2.4330 - acc: 0.8459BATCH GENERATED\n",
      "182/200 [==========================>...] - ETA: 16s - loss: 2.4196 - acc: 0.8468BATCH GENERATED\n",
      "183/200 [==========================>...] - ETA: 15s - loss: 2.4064 - acc: 0.8476BATCH GENERATED\n",
      "184/200 [==========================>...] - ETA: 14s - loss: 2.3933 - acc: 0.8484BATCH GENERATED\n",
      "185/200 [==========================>...] - ETA: 13s - loss: 2.3804 - acc: 0.849218\n",
      "BATCH GENERATED\n",
      "186/200 [==========================>...] - ETA: 13s - loss: 2.3676 - acc: 0.8501BATCH GENERATED\n",
      "187/200 [===========================>..] - ETA: 12s - loss: 2.3549 - acc: 0.8509BATCH GENERATED\n",
      "188/200 [===========================>..] - ETA: 11s - loss: 2.3424 - acc: 0.8517BATCH GENERATED\n",
      "189/200 [===========================>..] - ETA: 10s - loss: 2.3300 - acc: 0.8524BATCH GENERATED\n",
      "190/200 [===========================>..] - ETA: 9s - loss: 2.3177 - acc: 0.8532 BATCH GENERATED\n",
      "191/200 [===========================>..] - ETA: 8s - loss: 2.3056 - acc: 0.8540BATCH GENERATED\n",
      "192/200 [===========================>..] - ETA: 7s - loss: 2.2936 - acc: 0.8547BATCH GENERATED\n",
      "193/200 [===========================>..] - ETA: 6s - loss: 2.2817 - acc: 0.8555BATCH GENERATED\n",
      "194/200 [============================>.] - ETA: 5s - loss: 2.2699 - acc: 0.8562BATCH GENERATED\n",
      "195/200 [============================>.] - ETA: 4s - loss: 2.2583 - acc: 0.8570BATCH GENERATED\n",
      "196/200 [============================>.] - ETA: 3s - loss: 2.2468 - acc: 0.8577BATCH GENERATED\n",
      "197/200 [============================>.] - ETA: 2s - loss: 2.2383 - acc: 0.8582BATCH GENERATED\n",
      "198/200 [============================>.] - ETA: 1s - loss: 2.2271 - acc: 0.8589BATCH GENERATED\n",
      "199/200 [============================>.] - ETA: 0s - loss: 2.2159 - acc: 0.8596BATCH GENERATED\n",
      "200/200 [==============================] - 186s 932ms/step - loss: 2.2049 - acc: 0.8603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from time import time\n",
    "\n",
    "# callbacks\n",
    "monitor = 'loss'\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=os.path.join('./', model_name + '.hdf5'),\n",
    "    monitor=monitor,\n",
    "    save_best_only=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=monitor, factor = 0.3, patience=3)\n",
    "early_stopping = EarlyStopping(monitor=monitor, patience=5)\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=n_train_samples // batch_size,\n",
    "    epochs=2,\n",
    "    #validation_data=validation_generator,\n",
    "    #validation_steps=n_val_samples // batch_size\n",
    "    callbacks=[checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "print([1]+[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_test",
   "language": "python",
   "name": "kernel_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
